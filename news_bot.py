import requests
from bs4 import BeautifulSoup
import sqlite3
import asyncio
import re
import random
import time
from datetime import datetime, timedelta
import logging
from telegram import Bot
from telegram.error import TelegramError

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º–∏ –º–µ—Ç–∫–∞–º–∏
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class NewsBot:
    def __init__(self, token, channel_id):
        self.bot = Bot(token=token)
        self.channel_id = channel_id
        self.conn = sqlite3.connect('news.db', check_same_thread=False)
        self.cursor = self.conn.cursor()

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö
        self.update_database()

        self.news_queue = []
        self.last_posted_index = 0
        logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")

    def update_database(self):
        """–û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö"""
        try:
            self.cursor.execute('''CREATE TABLE IF NOT EXISTS posted_news
                                   (id TEXT PRIMARY KEY, 
                                    title TEXT,
                                    link TEXT UNIQUE,  
                                    added_time TIMESTAMP)''')
            self.conn.commit()
            logger.info("‚úÖ –ë–∞–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞/–æ–±–Ω–æ–≤–ª–µ–Ω–∞")
        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏—è –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö: {e}")

    def clean_text(self, text):
        """–û—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ –æ—Ç –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–±–∑–∞—Ü–µ–≤"""
        if not text:
            return ""

        # –†–∞–∑–¥–µ–ª—è–µ–º –Ω–∞ –∞–±–∑–∞—Ü—ã
        paragraphs = text.split('\n\n')
        cleaned_paragraphs = []

        for paragraph in paragraphs:
            if not paragraph.strip():
                continue

            # –£–±–∏—Ä–∞–µ–º –í–°–ï —Å—Å—ã–ª–∫–∏
            paragraph = re.sub(r'https?://\S+|www\.\S+', '', paragraph)

            # –£–±–∏—Ä–∞–µ–º —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –≤ –Ω–∞—á–∞–ª–µ –∞–±–∑–∞—Ü–∞
            paragraph = re.sub(r'^[A-Za-z–ê-–Ø–∞-—è—ë–Å\s]+/[A-Za-z–ê-–Ø–∞-—è—ë–Å\s]+/[A-Za-z–ê-–Ø–∞-—è—ë–Å\s]+,?\s*', '', paragraph)
            paragraph = re.sub(r'^[A-Za-z–ê-–Ø–∞-—è—ë–Å]+\s+[A-Za-z–ê-–Ø–∞-—è—ë–Å]+/,?\s*', '', paragraph)

            # –£–±–∏—Ä–∞–µ–º –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏ (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ —Å—Ç–æ—è—Ç –æ—Ç–¥–µ–ª—å–Ω–æ)
            sources_to_remove = [
                '¬©', '–í–µ–ª–µ–Ω–≥—É—Ä–∏–Ω –í–ª–∞–¥–∏–º–∏—Ä', 'Komsomolskaya Pravda', 'East News',
                '–†–ò–ê –ù–æ–≤–æ—Å—Ç–∏', '–¢–ê–°–°', 'Instagram', 'Meta', '–ò—Å—Ç–æ—á–Ω–∏–∫:',
                '–§–æ—Ç–æ:', '–í–∏–¥–µ–æ:', '–ú–∞—Ç–µ—Ä–∏–∞–ª –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª', '–ê–≤—Ç–æ—Ä:', '–¢–µ–∫—Å—Ç:',
            ]

            for source in sources_to_remove:
                # –£–±–∏—Ä–∞–µ–º —Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ —Å—Ç–æ–∏—Ç –æ—Ç–¥–µ–ª—å–Ω–æ
                paragraph = re.sub(r'\b' + re.escape(source) + r'\b', '', paragraph, flags=re.IGNORECASE)

            # –£–±–∏—Ä–∞–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å –∑–∞–ø—Ä–µ—â–µ–Ω–Ω—ã–º–∏ —Å–æ—Ü—Å–µ—Ç—è–º–∏
            patterns_to_remove = [
                r'\(–≤–ª–∞–¥–µ–ª–µ—Ü —Å–æ—Ü—Å–µ—Ç–∏ –∫–æ–º–ø–∞–Ω–∏—è Meta –ø—Ä–∏–∑–Ω–∞–Ω–∞ –≤ –†–æ—Å—Å–∏–∏ —ç–∫—Å—Ç—Ä–µ–º–∏—Å—Ç—Å–∫–æ–π –∏ –∑–∞–ø—Ä–µ—â–µ–Ω–∞\)',
                r'—Å–æ—Ü—Å–µ—Ç–∏.*?–∑–∞–ø—Ä–µ—â–µ–Ω–∞', r'Instagram.*?–∑–∞–ø—Ä–µ—â–µ–Ω', r'Meta.*?—ç–∫—Å—Ç—Ä–µ–º–∏—Å—Ç—Å–∫–æ–π',
            ]

            for pattern in patterns_to_remove:
                paragraph = re.sub(pattern, '', paragraph, flags=re.IGNORECASE)

            # –û—á–∏—Å—Ç–∫–∞ –æ—Ç –º—É—Å–æ—Ä–∞, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ–º –∑–Ω–∞–∫–∏ –ø—Ä–µ–ø–∏–Ω–∞–Ω–∏—è –¥–ª—è –∞–±–∑–∞—Ü–µ–≤
            paragraph = re.sub(r'[\[\]{}]', '', paragraph)
            paragraph = re.sub(r'<[^>]+>', '', paragraph)
            paragraph = re.sub(r'\S+@\S+', '', paragraph)
            paragraph = re.sub(r'\s+', ' ', paragraph)
            paragraph = paragraph.strip()

            # –£–±–∏—Ä–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è —Å–æ —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã (—Ç–æ–ª—å–∫–æ –µ—Å–ª–∏ –æ–Ω–∏ –≤ –∫–æ–Ω—Ü–µ)
            sentences = re.split(r'(?<=[.!?])\s+', paragraph)
            cleaned_sentences = []

            for sentence in sentences:
                sentence = sentence.strip()
                if sentence and len(sentence) > 15:
                    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –∫–æ—Ç–æ—Ä—ã–µ —è–≤–Ω–æ —è–≤–ª—è—é—Ç—Å—è —Å—Å—ã–ª–∫–∞–º–∏ –Ω–∞ –¥—Ä—É–≥–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã
                    lower_sentence = sentence.lower()
                    if not any(phrase in lower_sentence for phrase in [
                        '—Ä–∞–Ω–µ–µ –º—ã –ø–∏—Å–∞–ª–∏', '—á–∏—Ç–∞–π—Ç–µ —Ç–∞–∫–∂–µ', '—Å–º–æ—Ç—Ä–∏—Ç–µ —Ç–∞–∫–∂–µ',
                        '–ø–æ–¥—Ä–æ–±–Ω–æ—Å—Ç–∏ —á–∏—Ç–∞–π—Ç–µ', '–ø–æ—Ö–æ–∂–∏–µ –º–∞—Ç–µ—Ä–∏–∞–ª—ã', '–¥—Ä—É–≥–∏–µ –Ω–æ–≤–æ—Å—Ç–∏',
                        '–≤ –¥—Ä—É–≥–æ–π –Ω–æ–≤–æ—Å—Ç–∏', '–∫–∞–∫ —Å–æ–æ–±—â–∞–ª–æ—Å—å —Ä–∞–Ω–µ–µ'
                    ]):
                        cleaned_sentences.append(sentence)

            if cleaned_sentences:
                # –°–æ–±–∏—Ä–∞–µ–º –∞–±–∑–∞—Ü –æ–±—Ä–∞—Ç–Ω–æ
                cleaned_paragraph = ' '.join(cleaned_sentences)
                # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–µ–ª—ã –ø–æ—Å–ª–µ —Ç–æ—á–µ–∫
                cleaned_paragraph = re.sub(r'\.([–∞-—èa-z])', r'. \1', cleaned_paragraph)
                cleaned_paragraphs.append(cleaned_paragraph)

        # –°–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–±–∑–∞—Ü–µ–≤
        result = '\n\n'.join(cleaned_paragraphs)

        return result

    def truncate_at_sentence(self, text, max_length=900):
        """–û–±—Ä–µ–∑–∞–µ—Ç —Ç–µ–∫—Å—Ç –Ω–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–º –∑–∞–∫–æ–Ω—á–µ–Ω–Ω–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–∏"""
        if len(text) <= max_length:
            return text

        for end_char in ['.', '!', '?', '¬ª']:
            end_pos = text.rfind(end_char, 0, max_length)
            if end_pos != -1:
                if end_pos + 1 >= len(text) or text[end_pos + 1] in [' ', '\n', '\r', '\t']:
                    return text[:end_pos + 1].strip()

        last_space = text.rfind(' ', 0, max_length)
        if last_space != -1:
            return text[:last_space].strip() + '...'

        return text[:max_length].strip() + '...'

    def parse_news(self, limit=2):
        """–ü–∞—Ä—Å–∏–º –Ω–æ–≤–æ—Å—Ç–∏ —Å passion.ru - —Ç–æ–ª—å–∫–æ –¥–≤–µ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        sections = [
            "https://www.passion.ru/news/",
            "https://www.passion.ru/news/nash-shoubiz/",
            "https://www.passion.ru/news/eksklyuzivy/",
        ]

        all_news = []
        parsed_count = 0

        for section_url in sections:
            if parsed_count >= limit:
                break

            try:
                headers = {
                    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                }

                response = requests.get(section_url, headers=headers, timeout=15)
                soup = BeautifulSoup(response.text, 'html.parser')

                # –ò—â–µ–º –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ –∫–∞—Ä—Ç–æ—á–∫–∏
                news_items = soup.find_all(['article', 'div'],
                                           class_=lambda x: x and any(cls in str(x).lower() for cls in
                                                                      ['news', 'item', 'post', 'article', 'card',
                                                                       'story']),
                                           limit=limit)

                for item in news_items:
                    link = item.find('a', href=re.compile(r'/news/'))
                    if link and link.get('href'):
                        href = link['href']
                        title = link.get_text(strip=True)

                        if title and len(title) > 15:
                            if href.startswith('/'):
                                full_url = 'https://www.passion.ru' + href
                            else:
                                if 'passion.ru' not in href:
                                    continue
                                full_url = href

                            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
                            if not any(n['link'] == full_url for n in all_news):
                                all_news.append({
                                    'title': title,
                                    'link': full_url
                                })
                                parsed_count += 1
                                if parsed_count >= limit:
                                    break

                if parsed_count >= limit:
                    break

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ {section_url}: {e}")
                continue

        logger.info(f"üì∞ –ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(all_news)}")
        return all_news

    def parse_article_content(self, article_url):
        """–ü–∞—Ä—Å–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç —Å—Ç–∞—Ç—å–∏ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–±–∑–∞—Ü–µ–≤"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            }

            response = requests.get(article_url, headers=headers, timeout=25)
            soup = BeautifulSoup(response.text, 'html.parser')

            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for element in soup.find_all(['script', 'style', 'nav', 'footer', 'aside', 'form', 'iframe']):
                element.decompose()

            # –ò—â–µ–º –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–Ω—Ç–µ–Ω—Ç
            article_text = ""

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 1: –ò—â–µ–º —Ç–µ–≥ article
            article_tag = soup.find('article')
            if article_tag:
                paragraphs = article_tag.find_all('p')
                text_content = []
                for p in paragraphs:
                    text = p.get_text().strip()
                    if len(text) > 40:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –∞–±–∑–∞—Ü–∞
                        text_content.append(text)

                if text_content:
                    article_text = '\n\n'.join(text_content)

            # –°—Ç—Ä–∞—Ç–µ–≥–∏—è 2: –ò—â–µ–º –ø–æ –∫–ª–∞—Å—Å–∞–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞ (–±–æ–ª–µ–µ –º—è–≥–∫–∏–µ —É—Å–ª–æ–≤–∏—è)
            if len(article_text) < 200:
                content_classes = ['article-content', 'post-content', 'news-content', 'text-content']
                for class_name in content_classes:
                    content_blocks = soup.find_all('div', class_=lambda x: x and class_name in x)
                    for block in content_blocks:
                        paragraphs = block.find_all('p')
                        text_content = []
                        for p in paragraphs:
                            text = p.get_text().strip()
                            if len(text) > 30:
                                text_content.append(text)

                        if text_content:
                            article_text = '\n\n'.join(text_content)
                            break
                    if article_text:
                        break

            # –¢—â–∞—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º –∞–±–∑–∞—Ü–µ–≤
            article_text = self.clean_text(article_text)

            # –ò—â–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            photo_url = None
            meta_image = soup.find('meta', property='og:image')
            if meta_image and meta_image.get('content'):
                photo_url = meta_image['content']
                if photo_url.startswith('//'):
                    photo_url = 'https:' + photo_url
                elif photo_url.startswith('/'):
                    photo_url = 'https://www.passion.ru' + photo_url

            logger.info(f"üìÑ –î–ª–∏–Ω–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞: {len(article_text)} —Å–∏–º–≤–æ–ª–æ–≤")
            return article_text, photo_url

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ {article_url}: {e}")
            return None, None

    def is_posted(self, link):
        """–ü—Ä–æ–≤–µ—Ä—è–µ–º, –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–∞ –ª–∏ –Ω–æ–≤–æ—Å—Ç—å –ø–æ —Å—Å—ã–ª–∫–µ"""
        self.cursor.execute("SELECT link FROM posted_news WHERE link=?", (link,))
        return self.cursor.fetchone() is not None

    def mark_as_posted(self, news_id, title, link):
        """–û—Ç–º–µ—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å –∫–∞–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—É—é"""
        try:
            now = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
            self.cursor.execute("INSERT INTO posted_news (id, title, link, added_time) VALUES (?, ?, ?, ?)",
                                (news_id, title, link, now))
            self.conn.commit()
        except sqlite3.IntegrityError:
            pass

    async def check_news(self):
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π"""
        logger.info("üîç –ù–ê–ß–ò–ù–ê–ï–ú –ü–†–û–í–ï–†–ö–£ –ù–û–í–û–°–¢–ï–ô...")

        news_items = self.parse_news()
        new_news_count = 0

        for news in news_items:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º news['link'] –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏, –∞ –Ω–µ —Ö–µ—à
            if self.is_posted(news['link']):
                logger.info(f"‚è≠Ô∏è –£–∂–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ: {news['title'][:30]}...")
                continue

            news_id = str(abs(hash(news['link'])))

            logger.info(f"üìñ –ü–∞—Ä—Å–∏–º: {news['title'][:40]}...")
            content, photo_url = self.parse_article_content(news['link'])

            if not content or len(content) < 150:
                logger.info(
                    f"‚ùå –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç ({len(content) if content else 0} —Å–∏–º–≤–æ–ª–æ–≤): {news['title'][:30]}...")
                continue

            if not photo_url:
                logger.info(f"‚ùå –ù–µ—Ç —Ñ–æ—Ç–æ: {news['title'][:30]}...")
                continue

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –∫–æ–Ω—Ç–µ–Ω—Ç
            truncated_content = self.truncate_at_sentence(content)

            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ—á–µ—Ä–µ–¥—å
            self.news_queue.append({
                'id': news_id,
                'title': news['title'],
                'link': news['link'],
                'content': truncated_content,
                'photo_url': photo_url
            })

            new_news_count += 1
            logger.info(f"üì• –î–æ–±–∞–≤–ª–µ–Ω–æ –≤ –æ—á–µ—Ä–µ–¥—å: {news['title'][:50]}...")

        logger.info(f"‚úÖ –ü–†–û–í–ï–†–ö–ê –ó–ê–í–ï–†–®–ï–ù–ê. –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π: {new_news_count}")
        return new_news_count > 0

    async def publish_news(self):
        """–ü—É–±–ª–∏–∫–∞—Ü–∏—è –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –æ—á–µ—Ä–µ–¥–∏"""
        if not self.news_queue:
            logger.info("üì≠ –û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞")
            return False

        news = self.news_queue.pop(0)  # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –Ω–æ–≤–æ—Å—Ç—å –∏–∑ –æ—á–µ—Ä–µ–¥–∏ –∏ —É–¥–∞–ª—è–µ–º –µ–µ
        logger.info(f"üéØ –ü–û–î–ì–û–¢–û–í–ö–ê –ö –ü–£–ë–õ–ò–ö–ê–¶–ò–ò: {news['title'][:40]}...")

        try:
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            clean_content = self.clean_text(news['content'])

            message = f"<b>{news['title']}</b>\n\n{clean_content}\n\n#–Ω–æ–≤–æ—Å—Ç–∏"

            # –ü—ã—Ç–∞–µ–º—Å—è –æ—Ç–ø—Ä–∞–≤–∏—Ç—å —Å —Ñ–æ—Ç–æ
            try:
                await self.bot.send_photo(
                    chat_id=self.channel_id,
                    photo=news['photo_url'],
                    caption=message[:1024],
                    parse_mode='HTML'
                )
                logger.info("‚úÖ –ü–û–°–¢ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù –° –§–û–¢–û")

            except Exception as photo_error:
                logger.warning(f"‚ùå –û—à–∏–±–∫–∞ —Ñ–æ—Ç–æ: {photo_error}")
                await self.bot.send_message(
                    chat_id=self.channel_id,
                    text=message[:4096],
                    parse_mode='HTML'
                )
                logger.info("‚úÖ –ü–û–°–¢ –û–ü–£–ë–õ–ò–ö–û–í–ê–ù –ë–ï–ó –§–û–¢–û")

            # –û—Ç–º–µ—á–∞–µ–º –∫–∞–∫ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–Ω—É—é
            self.mark_as_posted(news['id'], news['title'], news['link'])

            return True

        except Exception as e:
            logger.error(f"‚ùå –û–®–ò–ë–ö–ê –ü–£–ë–õ–ò–ö–ê–¶–ò–ò: {e}")
            return False


# --- –ù–û–í–´–ô –ö–õ–ê–°–° –î–õ–Ø –£–ü–†–ê–í–õ–ï–ù–ò–Ø –†–ê–°–ü–ò–°–ê–ù–ò–ï–ú ---
class NewsScheduler:
    def __init__(self, news_bot):
        self.news_bot = news_bot
        self.working_start_hour = 9
        self.working_end_hour = 21

    async def run(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª —Ä–∞–±–æ—Ç—ã –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞"""
        logger.info("üöÄ –ü–õ–ê–ù–ò–†–û–í–©–ò–ö –ó–ê–ü–£–©–ï–ù!")

        while True:
            now = datetime.now()
            current_hour = now.hour

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω–∞—Ö–æ–¥–∏—Ç—Å—è –ª–∏ —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è –≤ —Ä–∞–±–æ—á–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª–µ
            if self.working_start_hour <= current_hour < self.working_end_hour:

                # –ï—Å–ª–∏ –æ—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞, –∑–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π
                if not self.news_bot.news_queue:
                    logger.info("üì≠ –û—á–µ—Ä–µ–¥—å –ø—É—Å—Ç–∞. –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É...")
                    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–æ–≤–µ—Ä–∫—É, –ø–æ–∫–∞ –Ω–µ –Ω–∞–π–¥–µ–º –Ω–æ–≤–æ—Å—Ç–∏
                    while not await self.news_bot.check_news():
                        logger.info("üòî –ù–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–µ—Ç. –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ 10 –º–∏–Ω—É—Ç...")
                        await asyncio.sleep(600)  # –ñ–¥–µ–º 10 –º–∏–Ω—É—Ç

                # –ü—É–±–ª–∏–∫—É–µ–º 2 –Ω–æ–≤–æ—Å—Ç–∏
                for i in range(2):
                    logger.info(f"üéØ –ù–∞—á–∏–Ω–∞–µ–º –ø—É–±–ª–∏–∫–∞—Ü–∏—é –Ω–æ–≤–æ—Å—Ç–∏ #{i + 1}...")

                    if self.news_bot.news_queue:
                        await self.news_bot.publish_news()
                    else:
                        logger.info("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –¥–ª—è –ø—É–±–ª–∏–∫–∞—Ü–∏–∏.")
                        break  # –í—ã—Ö–æ–¥–∏–º –∏–∑ —Ü–∏–∫–ª–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏, –µ—Å–ª–∏ –Ω–µ—Ç –Ω–æ–≤–æ—Å—Ç–µ–π

                    # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø—É–±–ª–∏–∫–∞—Ü–∏—è–º–∏ (2-3 –º–∏–Ω—É—Ç—ã)
                    if i < 1:
                        sleep_time = random.randint(120, 180)
                        logger.info(f"‚è∏Ô∏è –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—É–±–ª–∏–∫–∞—Ü–∏–µ–π –Ω–∞ {sleep_time // 60} –º–∏–Ω—É—Ç...")
                        await asyncio.sleep(sleep_time)

                # –û–∂–∏–¥–∞–µ–º –¥–æ –Ω–∞—á–∞–ª–∞ —Å–ª–µ–¥—É—é—â–µ–≥–æ —á–∞—Å–∞
                now = datetime.now()
                next_hour = now.replace(minute=0, second=0, microsecond=0) + timedelta(hours=1)
                sleep_duration = (next_hour - now).total_seconds()
                logger.info(
                    f"‚è≥ –ü—É–±–ª–∏–∫–∞—Ü–∏–∏ –∑–∞ —ç—Ç–æ—Ç —á–∞—Å –∑–∞–≤–µ—Ä—à–µ–Ω—ã. –°–ª–µ–¥—É—é—â–∞—è —Å–µ—Ä–∏—è –Ω–∞—á–Ω–µ—Ç—Å—è —á–µ—Ä–µ–∑ {sleep_duration // 60:.0f} –º–∏–Ω—É—Ç.")
                await asyncio.sleep(sleep_duration)
            else:
                # –í –Ω–µ—Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è –ø—Ä–æ—Å—Ç–æ –∂–¥–µ–º –¥–æ 09:00
                logger.info("üåô –ù–µ—Ä–∞–±–æ—á–µ–µ –≤—Ä–µ–º—è. –û–∂–∏–¥–∞–µ–º –¥–æ 09:00...")
                now = datetime.now()
                next_run_time = now.replace(hour=self.working_start_hour, minute=0, second=0, microsecond=0)
                if now.hour >= self.working_end_hour:
                    next_run_time += timedelta(days=1)

                sleep_duration = (next_run_time - now).total_seconds()
                logger.info(
                    f"‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ: {sleep_duration // 3600:.0f} —á–∞—Å–æ–≤ {(sleep_duration % 3600) // 60:.0f} –º–∏–Ω—É—Ç.")
                await asyncio.sleep(sleep_duration)


# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
BOT_TOKEN = "8352655660:AAGLuE9ee_qNFimaYWHPdakCw_57kTIfAcI"
CHANNEL_ID = -1002989870351


async def main():
    """–ó–∞–ø—É—Å–∫ –±–æ—Ç–∞"""
    try:
        bot = NewsBot(BOT_TOKEN, CHANNEL_ID)
        scheduler = NewsScheduler(bot)
        await scheduler.run()
    except KeyboardInterrupt:
        logger.info("üõë –ë–û–¢ –û–°–¢–ê–ù–û–í–õ–ï–ù –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–ï–ú")
    except Exception as e:
        logger.error(f"‚ùå –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")


if __name__ == "__main__":
    asyncio.run(main())